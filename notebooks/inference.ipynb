{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.24it/s]\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluation Metrics ===\n",
      "MSE            : 1.8767\n",
      "MAE            : 0.9094\n",
      "Accuracy       : 39.60%\n",
      "Pearson r      : 0.4807\n",
      "Spearman rho   : 0.4415\n",
      "Kendall tau    : 0.3873\n"
     ]
    }
   ],
   "source": [
    "%run ../train/inference_absolute.py \\\n",
    "    --model_dir /project/pi_wenlongzhao_umass_edu/1/jkarnuthala/experiment_results/prometheusv2/helpsteer/sft_model_quantized_new/sft \\\n",
    "    --test_csv /project/pi_wenlongzhao_umass_edu/1/experiment_results/prometheusv2/helpsteer/test_embeddings_with_scores_embeddings.csv \\\n",
    "    --output_csv /project/pi_wenlongzhao_umass_edu/1/jkarnuthala/experiment_results/sft_model/predictions.csv \\\n",
    "    --batch_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jkarnuthala_umass_edu/.conda/envs/offgenai/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:30<00:00,  7.65s/it]\n",
      "/home/jkarnuthala_umass_edu/.conda/envs/offgenai/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/jkarnuthala_umass_edu/.conda/envs/offgenai/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluation Metrics ===\n",
      "MSE            : 2.6602\n",
      "MAE            : 1.1214\n",
      "Accuracy       : 37.62%\n",
      "Pearson r      : 0.1194\n",
      "Spearman rho   : 0.0939\n",
      "Kendall tau    : 0.0844\n"
     ]
    }
   ],
   "source": [
    "%run ../train/inference_absolute.py \\\n",
    "    --model_dir /project/pi_wenlongzhao_umass_edu/1/jkarnuthala/experiment_results/llama3.18b/helpsteer/sft_model_quantized_error/sft \\\n",
    "    --test_csv /project/pi_wenlongzhao_umass_edu/1/experiment_results/llama3.18b/helpsteer/test_embeddings_with_scores_embeddings.csv \\\n",
    "    --output_csv /project/pi_wenlongzhao_umass_edu/1/jkarnuthala/experiment_results/sft_model/predictions.csv \\\n",
    "    --batch_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jkarnuthala_umass_edu/.conda/envs/offgenai/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.56it/s]\n",
      "/home/jkarnuthala_umass_edu/.conda/envs/offgenai/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/jkarnuthala_umass_edu/.conda/envs/offgenai/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluation Metrics ===\n",
      "MSE            : 2.1840\n",
      "MAE            : 1.1855\n",
      "Accuracy       : 22.54%\n",
      "Pearson r      : 0.4754\n",
      "Spearman rho   : 0.4313\n",
      "Kendall tau    : 0.3642\n"
     ]
    }
   ],
   "source": [
    "%run ../train/inference_absolute.py \\\n",
    "    --model_dir /project/pi_wenlongzhao_umass_edu/1/jkarnuthala/experiment_results/llama3.18b/summ_from_feedback/sft_model_quantized_new/sft \\\n",
    "    --test_csv /project/pi_wenlongzhao_umass_edu/1/experiment_results/llama3.18b/summarize_from_feedback/test_embeddings_with_scores_embeddings.csv \\\n",
    "    --output_csv /project/pi_wenlongzhao_umass_edu/1/jkarnuthala/experiment_results/sft_model/predictions.csv \\\n",
    "    --batch_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:03<00:00, 21.06s/it]\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluation Metrics ===\n",
      "MSE            : 2.3845\n",
      "MAE            : 1.1763\n",
      "Accuracy       : 27.25%\n",
      "Pearson r      : 0.5182\n",
      "Spearman rho   : 0.4627\n",
      "Kendall tau    : 0.3829\n"
     ]
    }
   ],
   "source": [
    "%run ../train/inference_absolute.py \\\n",
    "    --model_dir /project/pi_wenlongzhao_umass_edu/1/jkarnuthala/experiment_results/prometheusv2/summ_from_feedback/sft_model_quantized_new/sft \\\n",
    "    --test_csv /project/pi_wenlongzhao_umass_edu/1/experiment_results/prometheusv2/summarize_from_feedback/test_embeddings_with_scores_embeddings.csv \\\n",
    "    --output_csv /project/pi_wenlongzhao_umass_edu/1/jkarnuthala/experiment_results/sft_model/predictions.csv \\\n",
    "    --batch_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jkarnuthala_umass_edu/.conda/envs/offgenai/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:03<00:00, 21.13s/it]\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluation Metrics ===\n",
      "Accuracy       : 95.4037%\n",
      "Precision      : 96.2644%\n",
      "Recall         : 92.7978%\n",
      "F1 Score       : 94.4993%\n",
      "Pearson r      : 0.9060\n",
      "Spearman rho   : 0.9060\n",
      "Kendall tau    : 0.9060\n"
     ]
    }
   ],
   "source": [
    "%run ../train/inference_relative.py \\\n",
    "    --model_dir /project/pi_wenlongzhao_umass_edu/1/jkarnuthala/experiment_results/prometheusv2/offset_bias/sft_model_quantized/sft \\\n",
    "    --test_csv /project/pi_wenlongzhao_umass_edu/1/experiment_results/prometheusv2/offset_bias/relative_test_embeddings_with_scores_embeddings.csv \\\n",
    "    --output_csv /project/pi_wenlongzhao_umass_edu/1/jkarnuthala/experiment_results/sft_model/predictions.csv \\\n",
    "    --batch_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jkarnuthala_umass_edu/.conda/envs/offgenai/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.49it/s]\n",
      "/home/jkarnuthala_umass_edu/.conda/envs/offgenai/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/jkarnuthala_umass_edu/.conda/envs/offgenai/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No result found:\n",
      "No result found:\n",
      "No result found:\n",
      "No result found:\n",
      "No result found:\n",
      "No result found:\n",
      "No result found:\n",
      "=== Evaluation Metrics ===\n",
      "Accuracy       : 94.6397%\n",
      "Precision      : 94.6648%\n",
      "Recall         : 93.1359%\n",
      "F1 Score       : 93.8942%\n",
      "Pearson r      : 0.8913\n",
      "Spearman rho   : 0.8913\n",
      "Kendall tau    : 0.8913\n"
     ]
    }
   ],
   "source": [
    "%run ../train/inference_relative.py \\\n",
    "    --model_dir /project/pi_wenlongzhao_umass_edu/1/jkarnuthala/experiment_results/llama3.18b/offset_bias/sft_model_quantized/sft \\\n",
    "    --test_csv /project/pi_wenlongzhao_umass_edu/1/experiment_results/llama3.18b/offset_bias/relative_test_embeddings_with_scores_embeddings.csv \\\n",
    "    --output_csv /project/pi_wenlongzhao_umass_edu/1/jkarnuthala/experiment_results/sft_model/predictions.csv \\\n",
    "    --batch_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
